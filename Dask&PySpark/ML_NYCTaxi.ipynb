{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.pyspark.python': 'python3', 'spark.pyspark.virtualenv.enabled': 'true', 'spark.pyspark.virtualenv.type': 'native', 'spark.pyspark.virtualenv.bin.path': '/usr/bin/virtualenv'}, 'proxyUser': 'jovyan', 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "No active sessions."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\": {\n",
    "        \"spark.pyspark.python\": \"python3\",\n",
    "        \"spark.pyspark.virtualenv.enabled\": \"true\",\n",
    "        \"spark.pyspark.virtualenv.type\":\"native\",\n",
    "        \"spark.pyspark.virtualenv.bin.path\":\"/usr/bin/virtualenv\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>0</td><td>application_1747596292980_0002</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-22-200.ec2.internal:20888/proxy/application_1747596292980_0002/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-29-26.ec2.internal:8042/node/containerlogs/container_1747596292980_0002_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas==1.0.5\n",
      "  Downloading https://files.pythonhosted.org/packages/af/f3/683bf2547a3eaeec15b39cef86f61e921b3b187f250fcd2b5c5fb4386369/pandas-1.0.5-cp37-cp37m-manylinux1_x86_64.whl (10.1MB)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib64/python3.7/site-packages (from pandas==1.0.5)\n",
      "Collecting python-dateutil>=2.6.1 (from pandas==1.0.5)\n",
      "  Downloading https://files.pythonhosted.org/packages/ec/57/56b9bcc3c9c6a792fcbaf139543cee77261f3651ca9da0c93f5c1221264b/python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229kB)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/site-packages (from pandas==1.0.5)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas==1.0.5)\n",
      "Installing collected packages: python-dateutil, pandas\n",
      "Successfully installed pandas-1.0.5 python-dateutil-2.9.0.post0\n",
      "\n",
      "Collecting scipy==1.4.1\n",
      "  Downloading https://files.pythonhosted.org/packages/dd/82/c1fe128f3526b128cfd185580ba40d01371c5d299fcf7f77968e22dfcc2e/scipy-1.4.1-cp37-cp37m-manylinux1_x86_64.whl (26.1MB)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib64/python3.7/site-packages (from scipy==1.4.1)\n",
      "Installing collected packages: scipy\n",
      "Successfully installed scipy-1.4.1\n",
      "\n",
      "Collecting matplotlib==3.2.1\n",
      "  Downloading https://files.pythonhosted.org/packages/b2/c2/71fcf957710f3ba1f09088b35776a799ba7dd95f7c2b195ec800933b276b/matplotlib-3.2.1-cp37-cp37m-manylinux1_x86_64.whl (12.4MB)\n",
      "Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 (from matplotlib==3.2.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/e5/0c/0e3c05b1c87bb6a1c76d281b0f35e78d2d80ac91b5f8f524cebf77f51049/pyparsing-3.1.4-py3-none-any.whl (104kB)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /mnt/tmp/1747597318834-0/lib/python3.7/site-packages (from matplotlib==3.2.1)\n",
      "Requirement already satisfied: numpy>=1.11 in /usr/local/lib64/python3.7/site-packages (from matplotlib==3.2.1)\n",
      "Collecting cycler>=0.10 (from matplotlib==3.2.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/5c/f9/695d6bedebd747e5eb0fe8fad57b72fdf25411273a39791cde838d5a8f51/cycler-0.11.0-py3-none-any.whl\n",
      "Collecting kiwisolver>=1.0.1 (from matplotlib==3.2.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/f9/77/e3046bf19720b22e3e0b7c12e28f6f2c0c18a213fb91a56cea640862270f/kiwisolver-1.4.5-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.1MB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil>=2.1->matplotlib==3.2.1)\n",
      "Collecting typing-extensions; python_version < \"3.8\" (from kiwisolver>=1.0.1->matplotlib==3.2.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/ec/6b/63cc3df74987c36fe26157ee12e09e8f9db4de771e0f3404263117e75b95/typing_extensions-4.7.1-py3-none-any.whl\n",
      "Installing collected packages: pyparsing, cycler, typing-extensions, kiwisolver, matplotlib\n",
      "Successfully installed cycler-0.11.0 kiwisolver-1.4.5 matplotlib-3.2.1 pyparsing-3.1.4 typing-extensions-4.7.1"
     ]
    }
   ],
   "source": [
    "sc.install_pypi_package(\"pandas==1.0.5\", \"https://pypi.org/simple\")\n",
    "sc.install_pypi_package(\"scipy==1.4.1\", \"https://pypi.org/simple\")\n",
    "sc.install_pypi_package(\"matplotlib==3.2.1\", \"https://pypi.org/simple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import Window\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import (\n",
    "    VectorAssembler, OneHotEncoder, StringIndexer,\n",
    "    Bucketizer, PolynomialExpansion\n",
    ")\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = spark.read.parquet('s3://css-uchicago/nyc-tlc/trip_data/yellow_tripdata_2015*.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Columns: 19\n",
      "Total Rows: 146039231\n",
      "root\n",
      " |-- VendorID: long (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp (nullable = true)\n",
      " |-- passenger_count: long (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: long (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: long (nullable = true)\n",
      " |-- DOLocationID: long (nullable = true)\n",
      " |-- payment_type: long (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: integer (nullable = true)\n",
      " |-- airport_fee: integer (nullable = true)"
     ]
    }
   ],
   "source": [
    "print('Total Columns: %d' % len(data.dtypes))\n",
    "print('Total Rows: %d' % data.count())\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I identify two sets of special zones—airport and midtown—by hard-coding their TLC LocationIDs (`[236, 237]` for JFK/LGA and `[161, 162, 163, 186]` for Midtown). Trips touching these areas tend to follow very different distance, surcharge, and tipping behaviors than the rest of the city, so it makes sense to mark them out explicitly.\n",
    "\n",
    "Next, I build a new `corridor_id` string by concatenating the pickup and dropoff zone IDs (for example, `\"142_236\"`). This single categorical variable captures origin → destination pairs in one fell swoop, allowing the regression to learn separate coefficients for common flows—such as Uptown→JFK versus Midtown→JFK—rather than smearing all long trips together.\n",
    "\n",
    "To further isolate airport behavior, I add a binary `is_airport_trip` flag that is set to 1 whenever either endpoint is in an airport zone. In a similar spirit, `is_midtown_trip` flags journeys that begin or end in the dense Midtown taxi network. Both of these broad-strokes indicators pick up on the distinct fare structures, demand surges, and tipping norms typical of airport shuttles and Midtown short hops.\n",
    "\n",
    "Because `corridor_id` can take on hundreds (or thousands) of unique values, I convert it into a numeric form that my ML pipeline can digest: first with a `StringIndexer` (assigning each corridor a stable integer index), and then with a `OneHotEncoder` to turn that index into a sparse binary vector. This approach gives each high-volume OD pair its own parameter, while exploiting sparsity to keep computation tractable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# 1) Define any special zone lists\n",
    "airport_zones = [236, 237]               \n",
    "midtown_zones = [161, 162, 163, 186]     \n",
    "\n",
    "# 2) Build corridor_id and the two binary flags\n",
    "data = (data\n",
    "    # origin–destination corridor as “PU_DO”\n",
    "    .withColumn(\"corridor_id\",\n",
    "        F.concat_ws(\"_\", F.col(\"PULocationID\"), F.col(\"DOLocationID\"))\n",
    "    )\n",
    "    # airport‐trip flag\n",
    "    .withColumn(\"is_airport_trip\",\n",
    "        F.when(\n",
    "            F.col(\"PULocationID\").isin(airport_zones) |\n",
    "            F.col(\"DOLocationID\").isin(airport_zones),\n",
    "            1\n",
    "        ).otherwise(0)\n",
    "    )\n",
    "    # midtown‐trip flag\n",
    "    .withColumn(\"is_midtown_trip\",\n",
    "        F.when(\n",
    "            F.col(\"PULocationID\").isin(midtown_zones) |\n",
    "            F.col(\"DOLocationID\").isin(midtown_zones),\n",
    "            1\n",
    "        ).otherwise(0)\n",
    "    )\n",
    ")\n",
    "\n",
    "# 3) Categorical → index → one-hot for that corridor_id\n",
    "corridor_indexer = StringIndexer(\n",
    "    inputCol  = \"corridor_id\",\n",
    "    outputCol = \"corridor_idx\",\n",
    "    handleInvalid=\"keep\"\n",
    ")\n",
    "\n",
    "corridor_encoder = OneHotEncoder(\n",
    "    inputCols  = [\"corridor_idx\"],\n",
    "    outputCols = [\"corridor_ohe\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To capture the strong temporal rhythms in tipping behavior, I begin by extracting relevant time-based features from the original `tpep_pickup_datetime` column. I rename this timestamp to `pickup_ts` for clarity and ease of use in subsequent transformations.\n",
    "\n",
    "From this timestamp, I derive the hour of day using the `hour()` function and cast it to an integer. This `hour` feature is useful for identifying daily cycles, such as morning or evening rush periods when tipping may spike due to higher demand or driver scarcity.\n",
    "\n",
    "I also extract the day of week using `dayofweek()`, which returns values ranging from 1 (Sunday) to 7 (Saturday). However, this format is not aligned with the ISO standard, which defines Monday as 1 and Sunday as 7. To resolve this, I re-index the day values using modular arithmetic to produce a `weekday_iso` column that conforms to the ISO standard. This lets me consistently identify weekdays and weekends in downstream analyses.\n",
    "\n",
    "Building on this, I define an `is_weekend` binary feature that is set to 1 if the trip occurred on a Saturday or Sunday. Since tipping norms and rider behavior often shift between weekends and weekdays, this indicator helps the model adjust expectations accordingly.\n",
    "\n",
    "Finally, I introduce a `rush_hour` flag for trips that occur during the late afternoon peak (4–7 PM). This feature signals times of higher congestion, limited driver supply, and potentially longer trip durations—all of which may influence fare size and tipping propensity. These temporal features allow the model to capture predictable, time-linked variation in tipping behavior without needing to memorize individual timestamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = (\n",
    "    data\n",
    "    .withColumn(\"pickup_ts\", F.col(\"tpep_pickup_datetime\"))\n",
    "    .withColumn(\"hour\",       F.hour(\"pickup_ts\").cast(\"int\"))\n",
    "    .withColumn(\"dow_sun1\",   F.dayofweek(\"pickup_ts\").cast(\"int\"))\n",
    "    .withColumn(\n",
    "        \"weekday_iso\",\n",
    "        ((F.col(\"dow_sun1\") + F.lit(5)) % F.lit(7) + F.lit(1)).cast(\"int\")\n",
    "    )\n",
    "    .withColumn(\"is_weekend\", (F.col(\"weekday_iso\") >= 6).cast(\"int\"))\n",
    "    .withColumn(\"rush_hour\",  F.when(F.col(\"hour\").between(16,19), 1).otherwise(0))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = data.withColumn(\n",
    "    \"passenger_count_dbl\",\n",
    "    F.col(\"passenger_count\").cast(\"double\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I introduce non‐linear representations of trip distance to allow the model to learn different tipping patterns for short, mid‐range, long, and very‐long trips. First, I define `distance_splits` to separate the continuous `trip_distance` into four intuitive buckets—short (≤2 mi), mid (2–8 mi), long (8–15 mi), and very‐long (>15 mi)—and apply Spark’s `Bucketizer` to generate a new categorical feature `dist_bin`. This bucketization captures the “sweet spot” in the mid‐range where riders often tip more generously, without forcing a purely linear assumption.\n",
    "\n",
    "Next, I compute a `fare_per_mile` feature by dividing the total fare by the trip distance (when distance is positive) and defaulting to zero otherwise. This ratio conveys how expensive each mile of the trip was, helping the model distinguish, for example, short rides with high per‐mile fares (like airport trips) from long trips with lower unit costs. Because fare structure and rider generosity can vary markedly by trip economy, this normalized feature adds valuable granularity.\n",
    "\n",
    "Finally, I create an interaction term `dist_times_pax` by multiplying the raw trip distance by `passenger_count`. This captures how group size compounds distance effects—for instance, multi‐passenger rides may behave differently over the same distance than solo trips. By including this floating‐point interaction, I ensure the model can adjust its distance sensitivity based on party size rather than treating every mile as “worth” the same tipping potential regardless of how many people are on board."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 3) FLOATING-POINT  ──────\n",
    "#    nonlinear distance effects + fare / distance ratio\n",
    "# -------------------------------------------------------------\n",
    "# Bucketize trip_distance (non-linear bins) → categorical buckets\n",
    "distance_splits = [-float(\"inf\"), 2, 8, 15, float(\"inf\")]   # short, mid, long, very-long\n",
    "bucketizer = Bucketizer(\n",
    "    splits=distance_splits,\n",
    "    inputCol=\"trip_distance\",\n",
    "    outputCol=\"dist_bin\")                                  \n",
    "\n",
    "# fare per mile\n",
    "data = data.withColumn(\n",
    "    \"fare_per_mile\",\n",
    "    F.when(F.col(\"trip_distance\") > 0,\n",
    "           F.col(\"fare_amount\") / F.col(\"trip_distance\")\n",
    "    ).otherwise(0.0)\n",
    ")\n",
    "\n",
    "# interaction: distance × passenger_count  (integer × float)\n",
    "data = data.withColumn(\n",
    "    \"dist_times_pax\",\n",
    "    F.col(\"trip_distance\") * F.col(\"passenger_count\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I convert the continuous `passenger_count_dbl` into a categorical representation so that the model can learn distinct tipping behaviors across different party sizes. First, I apply Spark’s `StringIndexer` to map each unique passenger count (as a double) to an integer index in the new column `pax_idx`, ensuring that any unexpected or missing values are handled gracefully by keeping them as a separate category. Then, I use `OneHotEncoder` on `pax_idx` to produce a sparse vector `pax_ohe`, which allows the linear regression to assign an independent coefficient to each party‐size category rather than treating passenger count as a simple numeric input. This one‐hot encoding captures non‐linear shifts in tipping patterns between single riders, couples, and larger groups without imposing an arbitrary ordering.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pax_indexer = StringIndexer(\n",
    "    inputCol=\"passenger_count_dbl\",\n",
    "    outputCol=\"pax_idx\",\n",
    "    handleInvalid=\"keep\" \n",
    ")\n",
    "pax_encoder = OneHotEncoder(\n",
    "    inputCols=[\"pax_idx\"],\n",
    "    outputCols=[\"pax_ohe\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, I bring together all of the engineered columns into a single feature vector that can be consumed by the regression algorithm. I list out the untransformed numeric and binary flags—such as the cyclic hour (`hour`), weekend indicator (`is_weekend`), and rush‐hour flag (`rush_hour`)—alongside the continuous variables that capture distance and fare dynamics (`fare_per_mile`, `dist_times_pax`). I also include the spatial indicator (`is_airport_trip`), the one‐hot encoded passenger‐count vector (`pax_ohe`), and the bucketized distance category (`dist_bin`) to allow the model to learn non‐linear distance effects. Finally, I apply `VectorAssembler` to these columns, producing a new column called `features` that contains a single vector of all predictors. This consolidated vector simplifies downstream model training by presenting every engineered input in a uniform format.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "feature_cols = [\n",
    "    # temporal / binary\n",
    "    \"hour\", \"is_weekend\", \"rush_hour\",\n",
    "\n",
    "    # floating-point interactions\n",
    "    \"fare_per_mile\", \"dist_times_pax\",\n",
    "\n",
    "    # spatial flags\n",
    "    \"is_airport_trip\", \"is_midtown_trip\",\n",
    "\n",
    "    # nonlinear distance bucket\n",
    "    \"dist_bin\",\n",
    "\n",
    "    # high-cardinality OD pairs\n",
    "    \"corridor_ohe\",\n",
    "\n",
    "    # party-size one-hot\n",
    "    \"pax_ohe\",\n",
    "]\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=feature_cols,\n",
    "    outputCol=\"features\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I instantiate my core learner: a regularized linear regression that predicts tip_amount from the assembled feature vector. I set a modest number of iterations and apply both L1 and L2 penalties (elasticNetParam=0.2) to help control overfitting.\n",
    "\n",
    "With the learner defined, I assemble my full modeling pipeline. This chains together all of my feature transformers—indexing and encoding the passenger counts, bucketizing distances, vectorizing features—followed by the regression stage. Now I have one object that encapsulates every step from raw DataFrame to trained model.\n",
    "\n",
    "Before I even tune anything, I split the data once into an 80/20 train/test hold-out. This ensures that after cross-validation and model selection on the training partition, I still have an untouched subset on which to report a final, unbiased performance estimate.\n",
    "\n",
    "Finally, I configure a simple evaluator that knows to compare the \"prediction\" column against my true \"tip_amount\". Later on I’ll ask it for RMSE and any other regression metrics I choose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1) Define the regression learner\n",
    "lr = LinearRegression(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"tip_amount\",\n",
    "    maxIter=50,\n",
    "    regParam=0.1,\n",
    "    elasticNetParam=0.2\n",
    ")\n",
    "\n",
    "# 2) Build the full pipeline\n",
    "pipeline = Pipeline(stages=[\n",
    "    corridor_indexer, corridor_encoder,\n",
    "    pax_indexer, pax_encoder,\n",
    "    bucketizer,\n",
    "    assembler,\n",
    "    lr\n",
    "])\n",
    "\n",
    "# 3) Split into train / test once\n",
    "sampled_data = data.sample(fraction=0.01, seed=42)\n",
    "\n",
    "train_df, test_df = sampled_data.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# 4) Define evaluator\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"tip_amount\",\n",
    "    predictionCol=\"prediction\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I instantiate a CrossValidator, pointing it at my full pipeline (which already bundles all of my feature‐engineering steps and the linear regression estimator) and the evaluator that knows how to compute RMSE.  By setting numFolds=3, I ask Spark to perform three‐fold cross validation, and with parallelism=2 I enable two models to be trained in parallel, speeding up the search.\n",
    "\n",
    "Next, I call crossval.fit(train_df), which kicks off the cross‐validation process.  Under the hood, Spark will split train_df into three folds, train on two of them and validate on the third, rotating through all combinations, and will track which combination of hyperparameters (if I’d provided any) yields the lowest validation RMSE.  The result, cvModel, contains both the full history of models and the single best pipeline.\n",
    "\n",
    "Finally, I extract cvModel.bestModel—the pipeline instance that achieved the lowest cross‐validation error—and apply it to my held‐out test_df.  After transforming the test set, I use the same evaluator to compute the RMSE on that hold‐out data, giving me an unbiased estimate of how well my tuned pipeline will perform in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best regParam      : 0.00\n",
      "Best elasticNetParam: 1.00\n",
      "Test-set RMSE       : 2.11"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "paramGrid = (ParamGridBuilder()\n",
    "    .addGrid(lr.regParam,      list(np.arange(0.0, 0.1, 0.01)))\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 1.0])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "crossval = CrossValidator(\n",
    "    estimator=pipeline,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=RegressionEvaluator(\n",
    "        labelCol=\"tip_amount\",\n",
    "        predictionCol=\"prediction\",\n",
    "        metricName=\"rmse\"       # default, but made explicit\n",
    "    ),\n",
    "    numFolds=5,\n",
    "    parallelism=8              # one task per core node\n",
    ")\n",
    "\n",
    "# 6) Run CV (this fits the pipeline on each fold & hyperparam combo)\n",
    "cvModel = crossval.fit(train_df)\n",
    "\n",
    "bestModel = cvModel.bestModel\n",
    "preds     = bestModel.transform(test_df)\n",
    "evaluator = RegressionEvaluator(labelCol=\"tip_amount\",\n",
    "                                predictionCol=\"prediction\",\n",
    "                                metricName=\"rmse\")\n",
    "test_rmse = evaluator.evaluate(preds)\n",
    "\n",
    "print(f\"Best regParam      : {bestModel.stages[-1]._java_obj.getRegParam():.2f}\")\n",
    "print(f\"Best elasticNetParam: {bestModel.stages[-1]._java_obj.getElasticNetParam():.2f}\")\n",
    "print(f\"Test-set RMSE       : {test_rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 features by |coefficient|:\n",
      "  • dist_bin             →  +0.6165\n",
      "  • pax_ohe              →  -0.4696\n",
      "  • is_weekend           →  -0.2001\n",
      "  • is_airport_trip      →  -0.1792\n",
      "  • is_midtown_trip      →  -0.0724\n",
      "  • rush_hour            →  +0.0581\n",
      "  • corridor_ohe         →  +0.0165\n",
      "  • hour                 →  +0.0048\n",
      "  • fare_per_mile        →  +0.0008\n",
      "  • dist_times_pax       →  -0.0000"
     ]
    }
   ],
   "source": [
    "# 10) Show top 10 features by absolute coefficient\n",
    "feature_names = assembler.getInputCols()\n",
    "coeffs        = bestModel.stages[-1].coefficients.toArray()\n",
    "feat_coeff    = list(zip(feature_names, coeffs))\n",
    "top_feats     = sorted(feat_coeff, key=lambda x: abs(x[1]), reverse=True)[:10]\n",
    "\n",
    "print(\"\\nTop 10 features by |coefficient|:\")\n",
    "for feat, c in top_feats:\n",
    "    print(f\"  • {feat:20s} →  {c:+.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
